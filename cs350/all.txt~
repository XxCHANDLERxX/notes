CS 350 Jan 12

A1 is posted but we haven't covered any of the material yet :P
A1 is on synchronization. We implement two sync primatives: a lock and a condition variable.
We also have the primitive of a semaphor(?) implemented for us. That is good example code!
Then we need to implement a simulated traffic problem.
 - Naive solution: one car in the intersection at a time is safe, but removes concurrency.
 - We should allow parralel lanes to both be in the intersection at a time.
 - We will have a list of rules outlining what is safe or unsafe.
 - Performance is one aspect. Fairness is another. Any one car shouldn't have to wait for too long.
   "Avoid someone yelling YOLO and jumping the gun in road rage."

Recall: Threads
A thread is an abstraction for sequential execution.
If you want more than one thing running, you need multiple threads.
With one CPU, you must context switch between threads.
Context switching involves:
 1. Scheduling the next thread "determine which thread to run next"
 2. Save the thread context (stack, registers)
 3. Restore the thread context of the other thread
Causes of a context switch:
 1. Thread calls thread_yield "you might want other threads to catch up" "being nice"
    "thread_yield while waiting for a resource isn't very efficient, you should sleep instead"
 2. A thread blocks - sleeps in an "await channel" until an event happens (e.g. a device driver retrieving a packet)
 3. Interrupts for preemption (a timer) when a thread is running for its full time quantum
 4. When a thread is done
 
 Picture a thread, t, running a "load, modify, store" operation:
 --> lw
     add
     sw
If an interrupt happens in the middle of the lw, it will let the load finish.
Next, execution jumps to an interrupt handler. (In OS/161, this is the Common Exception Handler)
We are still in thread 1 and using thread 1's stack.
Inside the interrupt handler, we:
 1. Save the thread context.
 2. Service the interrupt.
 3. Restore the context.
 4. Resume execution (start executing the next instruction).
 
Implementing Preemptive Scheduling
 - Maintain a running_time variable.
 - When running_time = q = 500t, raise an interrupt.
 - In that interrupt's handler, we call thread_yield on behalf of the thread to trigger a context switch.
 Note: jumpimg to an interrupt handler isn't a context switch, we aren't going to an "OS thread", we are still in the same thread.
 Question: can we improve performance of our system by changing the timer to raise interrupts once every 500ms instead
  of every 1ms?
 Answer: it's not that simple - threads that run for <500ms and call a thread_yield at, say, 490ms, would trigger a context
  switch to the other thread, which gets preempted after 10ms! Then thread 1 gets to run again for 490ms - not good!

Stack after voluntary context switch: context stored in "switchframe" on thread's stack.
Stack after Preemption: context caught in "trapframe", and if t == q, thread_Switch creates a switchframe.
Q: What's the difference betwseen trapframes and switchframes?
A: They are snapshots of context at different points in time.
   Switchframe saves state after the interrupt handling functions are called. The interrupt handler needs to clean up still.
   The trapframe saves the context of the user program exactly when the interrupt fires.
Note that we have to save more registers in a trapframe than in a switchframe

Example:
Thread 1 is currently running (stack 1 has no switchframe or trapframe).
We can deduce that thread 2 executed a voluntary context switch, because there is a switchframe but no trapframe in stack 2.
Suddenly, a timer interrupt!
 - Jump execution to the interrupt handler.
   Note: there is a special kernel register that keeps track of the return address (using the return_from_interrupt(?))
 - Handler immediately saves the thread context into a trap frame.
 - First, the handler determines the type of interrupt and whether a context switch is required.
 - If a context switch is required, we call thread_yield, which:
   1. saves a switchframe to stack 1.
   2. determines the next thread to run (with round robin in this system of two threads, it is thread 2)
   3. restore the context of thread 2 - returns to the execution of thread_yield and thread_switch which finish and return to the user's code
Then thread 2 calls thread_yield for a voluntary context switch. thread_yield is called, which calls switch_thread and creates a switchframe.
This context switch transfers back to thread 1, where the switchframe is restored, and thread_yield restores the trapframe before returning.

Implementing threads
 - OS/161's thread library uses a thread control block to track thread metadata.
 - In the code, /kern/include/thread.h has `struct thread { ... }`. This is our thread control block.
   - Look at this code and its comments. Note that each thread has a "kernel level stack", which we'll cover later.
   - We have thread name, wait channel name, thread state, and a bunch of other data here.

Review: MIPS Register Usage
R0, zero = always returns 0
R1, at = used by the assembler
R2, v0 = return value / system call number
R3, v1 = return value
R4, a0 = 1st argument (to subroutine)
R5, a1 = ...

t0 to t9 are saved by trapframe but not switchframe (?)
R16 to R31 must be saved (?)

Dispatching on the MIPS "saving the context and restoring the context"
 - See /kern/arch/mips/thread/switch.S
 - It saves the necessary registers that must be preserved across function calls.
 - Note that r7 is a "hack" in OS/161 that always points to the current thread structure, so it doesn't need to be saved.
 - It gets the new stack ponter from the new thread.
 - It then restores the registers. Note that restore offsets should be consistent with the save offsets.
Not all registers are saved during a context switch (not all the temporary registers).
In a trap frame, essentially all registers need to be saved (with the exception of r0 and kernel registers).
"Involuntary context switches thus take longer than voluntary context switches."
CS 350 Jan 14

Recall: First thing happens when an interrupt fires is to jump to the interrupt handler
in memory, which saves a trap frame, determines the type of interrupt and services it, etc.

switchframe_switch:
/*
a0 contains address of switchframe pointer of old thread
a1 contains address of switchframe pointer of new thread
thread2's control block has a pointer t_context, pointing to the switchframe on stack2.
*/
// 1. allocate stack space for saving 10 registers (10*4 = 40 bytes)
addi sp, sp, -40
// Save the registers
...
// Get new stack pointer from the new thread
lw sp, 0(a1)
// Restore the registers
lw s0, 0(sp)
lw s1, 4(sp)
...
lw ra, 36(sp)
nop
// return
j ra
addi sp, sp, 40 // in delay slot
.end switchfram_switch

The control block is stored by the thread library.
Later we will find out it's in the kernel (rather than the user space).
The kernel allocates some memory for the control blocks, etc.

Concurrency
When multiple threads need to talk to each other, synchronization problems surface.
e.g. Two programs accessing the same variable.
On multiprocessors. On uniprocessors, only one thread runs at a time, but preemption and
timesharing make it appear that threads are running concurrently.
However, concurrency and synchronization are important even on uniprocessors.
It's a common misconception that uniprocessors don't have synchronization issues.
Preemption still happens on uniprocessors.
Timesharing between threads for a certain time quantum are outside of the control of the
program, so from the perspective of the program, multiple threads are running at the same time.
Any synchronization problems you have on multiprocessors, you can have on uniprocessors!

Thread Synchronization
Concurrent threads can interact in a variety of ways:
 - sharing access to the system, devices, etc.
   "don't want multiple programs writing to the screenbuffer at the same time"
 - sharing access to program data, e.g. global variables
   "program data and code are shared by threads of the same programming"
   "we want mutual exclusion for global variable access between threads"

Mutual exclusion: Only one thread can access a global variable at a time.
Critical section: The part of a program in which the shared object is accessed.
"Only one thread should be in the critical section at a time."
"Memorize these two definitions."

Critical Section Example (Part 0)
int volatile total = 0;
void add(total, N) {...} // executed by thread1
void subtract(total, N) {...} // executed by thread2
When compiled, local loop counter i becomes "loadaddr R8 total" and "loadaddr R10 total".
Each i becomes a local copy of total that gets incremented/decremented.
Each thread operating independently increments and decrements R9.
Note that context switches preserve the value of R9 between the threads so that they each have
a consistent view of R9.
If thread2 writes to R9 before thread1 after one iteration, total will become -1, then 1. Not 0!
This leads to very unpredictable results.

The Volatile Keyword
Tells C that this global variable shouldn't be optimized.
Without volatile, the compiler could say, "I'll just move the load and store outside of the for
loop."
A smart compiler would just add N to the register rather than looping through and incrementing
the register N times. Trying to fix this becomes very difficult!
Volatile would tell the compiler not to perform optimizations on code that access that variable,
i.e. it leaves lw and sw inside of the loop as-is.
"Not the entire function remains unoptimized, just the part that accesses the global variable."
This lets us identify the critical section more easily.
This also slows down our code for the sake of correctness - repeated lw and sw are not fast.

Critical section: "anywhere in your code that accesses a shared variable"

// thread1 ... thread2
total++;       total--;
// critical section spans both code blocks between the two threads

Only one thread can be inside the critical section at each time.

Let's assume that there is only one critical section in each program.
i.e. Begins and end in the code exactly once, rather than hopping in and out.

Critical Section Example (Part 1)
list_add_front(list *lp) {...} // entire function is critical section due to access of list *lp
(Part 2)
list_append(list *lp, int new_item) {...} // in the same critical section as list_add_front due
// to potential conflicting access of list *lp

Error example: thread1 and thread2 executing critical section list_remove_front at the same time
What could go wrong?
1. both threads return the same value
2. decrements num_in_list twice even though only one item has been removed
3. dereference an element that has been freed
4. calling free() twice on the same element

Another error example: thread1 running list_remove_front and thread2 running list_append
thread2 `if(is_empty(lp)){lp->first=element;lp->last=element;}`
What if a context switch happens immediately after the boolean if-statement check,
and thread1 deletes the only element of lp?

The critical section includes the entirety of both functions.
Introducing locks would make the linked list safe to use across multiple threads.

Enforcing Mutual Exclusion
Several techniques:
 - (difficult) exploit special hardware-specific instructions
 - (simpler) control interrupts to ensure that threads are not preempted while they are executing a
   critical section
Disabling Interrupts
 - i.e. Disabling interrupts at the beginning of the critical section and re-enabling them after.
 - On a uniprocessor, violations happen when an involuntary context switch happens inside a critical
   section that switches to a thread that enters the same critical section.
 - Disabling interrupts makes this violation impossible (assuming thread_yield isn't called inside the
   critical section).
Q: Why can't a program arbitrarily disable interrupts and run forever?
A: Later we will learn that the kernel runs in a "privileged mode" (instead of user mode(?)) and
   only privileged mode can disable interrupts.

Interrupts in OS/161
This is one way that the kernel enforces mutual exclusion on a single processor. There is a simple
interface:
 - sp10() sets Interrupt Priority Level (IPL) to 0, enabling all interrupts
 - splhigh() sets IPL to highest value, disabling all interrupts
 - splx(s) sets IPL to s
So we can enable and disable interrupts by calling spl0() or splhigh(), respectively.

But this problem arrises:
A() {
    splhigh(); // disable interrupts
    ...
    B(); // enables interrupts :(
    ...
    spl0();
}
B() {
    splhigh(); // disable interrupts
    ...
    spl0(); // enable interrupts
}

Solution: keep a counter of the number of times interrupts are disabled and enabled, and only
re-enable interrupts when they have been enabled the number of times they were disabled.
These are used by splx() and by the spinlock code:
splraise(int oldipl, int newipl)
spllower(int oldipl, int newipl)
For splraise, NEWIPL > OLDIPL, and for spllower, NEWIPL < OLDIPL

This is a nice solution, because it doesn't require special hardware, and works for any number
of concurrent threads.
However, this is very indiscriminate - it stops all preemption, not just preemption that would threaten
the critical section. Ignoring timer interrupts also has side effects, i.e. ignoring the passage of time.
It also doesn't enforce mutual exclusion on multiprocessors (interrupts are only disabled for one core at
a time). Multiple cores means multiple threads running, even without context switching and interrupts.
CS350 Jan 19

Recall: `total++;` compiles to load, add, and store instructions.
A conflicting thread could be manipulating the same variable, resulting in a lost update,
so this code touching the shared variable total is in the critical section.
We use the `volatile` keyword to disable compiler optimization for a specific variable,
which makes it easier to recognize the critical section and helps towards mutual exclusion.

Mutual exclusion: Only one thread can access a shared variable at a time.
Critical section: All code that accesses a shared variable.
Simple implementation of mutual exclusion from last class: Disabling interrupts during the critical
section.
Three key downsides to disabling interrupts:
 1. Only works for one core.
 2. Ignoring the timer has side effects, such as losing track of time.
 3. It's very indescriminate - no context-switching can happen for everyone, even threads that don't
    touch the critical section.

How would we implement mutual exclusion in the real world?
Imagine you are trying to get inside the only bathroom stall in a bathroom.
When you walk into the bathroom, you check to see if someone is in the stall (it is locked).
If it's occupied, you wait (or "spin") until you see the door become unlocked before enterring.
Then you enter the stall (the critical section) and lock the door.

In pseudocode:
    while(lock==1){
        lock=1;
        ...
        lock=0;
    }

Problem: the lock variable is shared, so the same problem arrises.
Solution: Make `while(lock==1){lock=1;}` a single instruction, making it atomic.
Many instruction sets give us such an instruction, most commonly called TestAndSet.

Pseudocode:
TestAndSet(addr, value){
    old = *addr; // get old value of lock at addr
    *addr = value; // write new value to addr
    return old;
}

We can use this atomic instruction to implement locks.
The "lucky" program that acquires the lock gets a 0 returned, and can enter the critical section.
All other threads get a 1 returned, which says "the lock is still active".

In x86, we have the xchg instruction:
xchg src, dest
Where src is a register and dest is an address.

Alternatives to TestAndSet:
CompareAndSwap(addr,expected,value){
    old = *addr;
    if (old == expected) *addr = value;
    return old;
}
If used simply to build a spinlock, it's just as powerful as TestAndSet.
However, it is more powerful when solving synchronization problems without a lock.
For example:

void add(){ // works without locks implemented!
    int i;
    int expected = total;
    for(i=0; i<N; i++){
        while(1){
            int prev=expected;
            expected = CompareAndSwap(&total, expected, expected+1);
            if (expected==prev){
                break;
            }
        }
    }
}
"The above example isn't that important, it's jsut a glimpse at lock-free data structures."
"Take CS 343."

MIPS uses load-linked and store-conditional
Load-linked: returns the current value of a memory location, and a subsequent store-conditional
to the same memory location will store a new value only if no updates have occured since the
load-linked.

Starvation: When one thread of many fighting for a lock loses every time. In practice, it isn't
something to worry about, but it could theoretically happen when implementing spin locks.

A Spin Lock Using TestAndSet:
boolean volatile lock; // one lock variable for each critical section
while(TestAndSet(&lock, true)) {} // busy-wait until the thread enters critical section
lock = false; // when thread leaves the critical path

Spinlocks in OS/161
struct spinlock {...}
spinlock_init // 
spinlock_cleanup // make sure the lock value is 0 (nobody inside critical section) when you cleanup a spinlock
spinlock_data_set
spinlock_acquire(struct spinlock *lk){ // lots of code:
    splraise(IPL_NONE, IPL_HIGH);
    // disable interrupts
    //  - optimizes so that this function always acquires lock
    //  - interrupts firing inside acquire_spinlock causes problems because future calls to acquire_spinlock
    //    will fail, and since you're in the same thread trying to acquire the same lock we will get stuck
    while(1){
        // do Test-and-TestAndSet to reduce bus contention
        if(spinlock_data_get(&lk->lk_lock) != 0) {
            continue; // loop again now and avoid the expensive TestAndSet operation from saturating the memory bus
        }
        if(spinlock_data_testandset(&lk->lk_lock) != 0){
            continue;
        }
        break;
    }
}

TestAndSet saturates the memory bus:
Picture two CPUs accessing main memory to get a value.
They each store the value in their own CPU cache.
If one CPU modifies the value, it writes back the value to memory,
and invalidates the caches of the other CPU (via the memory bus again).
So when the other CPU wants the value again, it must use the memory bus to get the value again.
In our case, "accessing a variable" is calling TestAndSet on a shared value.
CPU0 calls TestAndSet and recieves a new value into its local cache.
CPU1 calls TestAndSet, which is a read and a write, invalidating CPU0's local cache.
They take turns invalidating each other.
The memory bus becomes the bottle neck, because CPU caches become useless.

test-and-TestAndSet avoids the above problem, because the initial test only uses a read operation,
in this case spinlock_data_get.
The only time the memory bus is used is after there is some type of state transition of the lock (0 to 1 or 1 to 0).

Implementing TestAndSet using only load_linked and store_conditional:

spinlock_data_testandset(volatile spinlock_data_t *sd){...} // see Synchronization slide 23, 24
"%0 represents the first local variable - load the value of the lock into the first local variable, x=*sd;"
ll %0, 0(%2) // x = *sd
sc %1, 0(%2) // *sd = y; y = success?
// if y=0, ll and sc failed, and some other thread modified the shared variable, unsafe to proceed => return 1
// returning 1 means that you were unable to acquire the lock (even if it's available, we cant ensure that)
// return 1; => try again
"You aren't absolutely sure if someone's in the washroom, so don't go in - check the lock again!"
"Someone fiddled with the lock."

spinlock_release // set value of the lock back to 0 and re-enable interrupts

Pros and Cons of a Spinlock:
+ fairly efficient
+ works with multiple CPUs
- CPU is busy while waiting for a lock (large critical sections will spin the CPU needlessly for a long time
  as it is only checking the lock)
- starvation is possible

Thread Blocking
If a thread needs to access a critical section that is busy, it must wait for the critical section to become free.
Let's say you want to go to bed at 11pm and want to wake up at 6pm. We would ideally use an alarm clock.
Spinlocks, though, would be like you are looking at the clock every second - very inefficient! No rest happens!
To handle this, the thread scheduler blocks threads - the thread stops running until signaled to wake up.CS 350 Jan 21

Recap: Motivations for TestAndSet atomic function, and related MIPS instructions.
TestAndSet(addr,value){
    old=*addr;
    *addr=value;
    return old;
}
Naively translated to lw, sw, and return in MIPS.
This won't work, because two calls to the same function will result in a missed
update. To do this correctly, we need to use LoadLink (ll) and StoreConditional (sc):
R2=1;
ll R1 0(addr); // R1=0 (not lw)
sc R2 0(addr); // not sw
if(R2==0) return 1; // "somebody could be inside the lock in this iteration, please try again"
return R1;

LoadLink sets a "watch" on R1, and StoreConditional tells us if the variable hasn't
been touched.

Recall: Thread Blocking
Thread sleeps while waiting for a resource, a lock, etc.
For OS/161, we use wait channels to implement thread blocking.
We will use wait channels in assignment 1!
waitchannel_lock: modify the wc structure safely by acquiring a wchan_lock
wchan_unlock: unlocks the spinlock; later we never really need to call this
wchan_sleep: puts calling thread to sleep, saves context similarly to thread_yield
 - instead of being added to a thread queue, the thread gets added to the wait channel's
   block queue
 - like sleeping beauty, she can't be woken up on her own, a condition must be met (prince charming!)
wchan_wakeone: wake one sleeping thread "usually the one that's waiting the longest" "if there's a line of sleeping beauties"
wchan_wakeall: wakes up everyone "like a fire alarm"
wchan_sleep: calling thread is voluntarily giving up the CPU

Important diagram: thread states
1. Running
 - becomes ready when quantum expires (involuntary context switch) or thread_yeild (voluntary)
 - becomes blocked when waiting for resource or event (echan_sleep())
2. Blocked
 - becomes running when a resource is free or event happens (wchan_wakeone/all())
3. Ready
 - runs when dispatched by the thread scheduler
"The scheduler should be the only one deciding which thread runs next. One point of reference."
Clarification: Scheduler is the only thing that determines which ready thread runs next, but different
parts of the thread library could be dispatching.

Semaphores (Also used in A1!)
A higher-level synchronization construct.
A spinlock and wait channel are all we need to solve synchronization problems, but they are low-level
and tricky to use.
A semaphore is a counter with two functions to manipulate it:
 - semaphore.P(): if counter > 0, semaphore--
 - semaphore.V(): semaphore++
"Just a way of managing a counter."
"Why is it P and V?" Djikstra implemented this, so the variables are Dutch. (?)
Counting semaphors: any non-negative value
Binary semaphore: 1 or 0

Mutual Exclusion Using a Semaphore
struct semaphore *s;
s = sem.create("MySem1", 1); // initial value is 1
P(s); // before critical section, decrement (if it can't, block this thread and wait)
// critical section (e.g. remove_front())
V(s); // after critical section, increment to free up the critical section and wake up 1 blocked thread
"V wakes up blocked threads and activates a ready one."

In add() and subtract() from before:
...
P(sem);
total++ or total--; // critical section
V(sem_;
...

OS/161 Semaphores
kern/include/synch.h and kern/thread/synch.h

struct semaphore {
    char *name;
    struct wchan *sem_chan;
    struct spinlock sem_lock; // protects variables in the semaphore structure
    struct spinlock sem_lock; // provides mutual exclusion to semaphore vars
    volatile int sem_count;
}

semaphore:P()
"Two assert statements: check the pointer exists, and check you aren't inside the interrupt handler."
"Never use semaphores inside our interrupt handlers so that we don't deadlock."
"Don't sleep with the lock and key!"
1. acquire spinlock
2. check the count value (greater than 0 => decrement, release lock, return)
3. if count == 0, block (wchan_sleep)
   particular order:
    - acquire wchan_lock
    - release spinlock
    "If we release the spinlock first, an interrupt could mess up our semaphore."
    "Always hold onto at least one lock (wchan or semaphore)."
    - wchan_sleep automatically releases the wchan_lock "so someone can come wake you up"
    - get woken up after sem.V(): re-acquire the spinlock so you can check and decrement the counter
    - very important to have a while loop here, NOT an if statement
    - check sem->sem_count==0 again, and sleep if it is 0 again
semaphore:V()
 - acquire spinlock
 - increment the count
 - wake one person (if there are no other threads, this does nothing)
 "Note that wakeone and wakeall automatically acquires and releases the wchan lock."
 "Only wchan_sleep doesn't know how to acquire the wchan lock."
 "We have to release the spinlock explicitly after acquiring the wchan lock in sem.P(), so we do it manually."
 - release the spinlock

Example: while loop is critical, can't be an if statement
Thread1 calls P(), context switches, Thread2 calls P(), fails and blocks.
Context switch to Thread3 for a while, then back to T1.
T1 calls V() and wakes up T2, adding it to the ready queue.
Context switch to T3, calls P() and enters the critical section before T2!

Example: spinlock is released before wchan_lock acquired
T1 calls P(), 0, blocks, eventually calls spinlock_release(), context switches to T2.
T2 calls V(), increments to 1, calls wchan_wakeone, eventually context switches to T1.
T1 calls wchan_lock and wchan_sleep - it missed the wakeone and will sleep forever.
"Before sleeping beauty sleeps, she leaves the room for a second, prince charming comes, doesn't see her, leaves,
 then she comes back and sleeps forever."

Example: (real implementation) wchan lock acquired before releasing spinlock
T1 calls P(), wchan_lock, spinlock_release(), context switch to T2.
T2 calls V(), calls wakeone, spins while trying to acquire wchan lock.
Eventual context switch to T1, T1 calls wchan_sleep.
T2 can now call wchan_wakeone, then spinlock_release.
T1 can now call spinlock_acquire.

Real-world example: Producer-Consumer model.
A number of threads are producing data (producers) and threads consuming the data.
The producers write data to a queue, and producers take it from the queue.
We want to avoid consumers going faster than producers and asking, "is something there?" continually
on an empty queue, wasting resources.
We want consumers to sleep until there is data to consume.
1. Initialize semaphore to 0 (the number of items currently in queue, a linked list or whatever).
2. When the producer creates an item, it calls semaphore.V() (and loops).
3. The consumer calls P() and then removes an item (and loops).
However, this implementation isn't currently safe, we need a guard lock for the list.
Add a binary semaphore: P() before adding/removing from the list, and V() after. (A critical section).
Note: this was an unbounded buffer.

Bounded buffer example: imagine producer is much faster than consumers, so the queue quickly fills up.
If the buffer is full, the producer(?) needs to sleep.
Two semaphores: occupied, set to 0, and unoccupied, set to N (# memory slots).
Producer loop:
1. P(unoccupied)
2. add item to list (list_append())
3. V(occupied)
Consumer's loop:
1. P(occupied) // initially 0, so it blocks until a producer creates data
2. remove item from list (list_remove_front())
3. V(unoccupied) // increments number of free spots available in the queue

OS/161 Locks (Implementing on A1!)
Similar to bin semaphore, except it also enforces that the same thread must call P() and V().
If some other thread tries to release the lock, it crashes the system or something severe.
That's the main difference between a blocking lock and a binary semaphore.

Condition variables (Implementing on A1!)
Like a light wrapper for a wait channel.
Works with a blocking lock.
wait: inside critical section, check condition, leave critical section, release blocking lock
another thread can enter the critical section, change your condition, wake you up
two variables: lock required to protect the critical section, and your condition variable
signal/broadcast: wake up one or more threads that are waiting for a condition, releases blocking lock
"only use condition variables inside the critical section"
CS 350 Jan 26

Recall: Two different types of semaphores, condition variables, solving the producer/consumer problem.
Blocking lock and binary semaphore are different because:
- Blocking locks can only be released by the thread that acquired the lock.
- This is because we just want to provide mutual exclusion, so we use this lock to surround our
  critical section.

Condition variable usage:
1. Acquire a lock before enterring critical section.
   The condition is probably a shared variable.
2. If the condition is false, block inside the critical section.
   Blocking is to momentarily step out of the critical section while releasing the lock.
   This avoids deadlock because the interrupt needs access to the lock to function correctly. (?)

Example: Stock Trading App
Tbuy (<$500)
Tsell (>$1000)
Tupdate
condition variables: cv_sell true if stock >1000, cv_buy true if <500.


Critical section visualization:

----|  |----------.
   lock_acquire   |
                  | cv_sell queue
                  '---------.
                   Tsell    |
                  -'''''''''
 int StockPrice   |
                  | cv_buy queue
                  '---------.
                   Tbuy     |
   lock_release   -'''''''''
-----|   |--------'
   
Tbuy has a critical section.
Entering the critical section: lock_acquire
Check the stock price (a shared variable - this is why we are in a critical section)
Tbuy checks the price and sees $700. It wants <$500, so we sleep.
Instead of exiting and trying over and over again (inefficient), we sleep until cv_buy
becomes true. So we call cv_wait(cv_buy) and "line up" in wait for that variable's signal.
By going to sleep, we temporarily step out of the critical section, so we must release the lock.
Problem: releasing the lock before going to sleep could lead to another thread changing the proce
and triggering cv_buy's signal before you go to sleep.
So we implement this the same way we implemented semaphores (see previous example).
i.e. We release the lock after exiting the critical section and entering the queue.

Important idea for A1: one critical section can have multiple blocking condition variables.

Now Tsell comes into the critical section.
 - it acquires the lock
 - it checks cv_sell condition, sees that it's still false
 - sleeps on cv_sell, stepping out of critical section and releasing the lock

Now Tupdate comes into the critical section and updates the stock from $700 to $1010.
Tupdate must now signal or broadcast cv_sell's blocked threads (should NOT broadcast to *all*
condition variables, that would be inefficient).
Tsell thread now becomes "ready", it can't run right away because:
 - it needs to be scheduled first
 - Tupdate thread still has lock on critical section

Note: The condition variables we are covering here are technically "Mesa-style". Other
styles of condition variables exist and behave slightly differently, but we won't be
covering them.

Code example: Synchronization slides 45-46.
Notice that Produce() and Consume() both call cv_wait() inside a while loop.
This is to confirm that the condition is true before breaking and leaving the while loop.
Note that the call to cv_wait is inside a while loop *after* we acquire a lock.
cv_wait automagically releases the lock, and regains it for you before returning.

Deadlocks
Suppose two locks, lockA and lockB, both initially unlocked.
Suppose these events occur:
1. Thread 1 does lock_acquire(lockA)
2. Thread 2 does lock_acquire(lockB)
3. Thread 1 does lock_acquire(lockB) and blocks, because thread 1 already has it.
4. Thread 2 does lock_acquire(lockA) and blocks, because thread 2 already has it.
Steps 3 and 4 will repeat themselves, so we are stuck.
Another thread can't step in to release one of the locks, because they must be released
by the thread that acquired it.

Another example of deadlocking:
64MB of memory.
1. ThreadA requests 30MB of memory.
2. ThreadB starts and requests 30MB of memory.
3. ThreadA requests an additional 8MB of memory, but the kernel blocks because only 4MB are left.
4. ThreadB requests an additional 8MB of memory, but the kernel blocks because only 4MB are left.

A real world example: Picture of intersection where cars from all four sides tried entering the
intersection but got stuck halfway when they hit against the other cars.
||
\/<---
--->/\
    ||
We can think about the intersection as a collection of 4 resources.
  |  |
-- AB --
-- CD --
  |  |
In order for a car to drive from the south to the north in the right lane, it needs resources B
and D. Note that naively acquiring resource D without resource B leads to the deadlock above.
Even though we don't necessarily need resource D AND B until we are halfway through the intersection,
we are at risk for getting stuck.
Note: this scenario is NOT related to A1, we should solve it in a much simpler way without using
"resources" in the intersection like this.

Ways to prevent deadlocks:
1. Prevent "hold and wait" scenarios, where a car (a thread) is holding onto a resource (D)
while waiting for another (B). Acquire the needed resources all at once, or drop your held resouce
and try again later if B can't be acquired.
This solution isn't complete, because we could end up with traffic from all sides simultaneously
pulling into the intersection, backing up, pulling back in, etc. This is a "live lock" situation.

Another "live lock" situation is a Canadian four-way stop where everyone tries to let the car on
there right go first - it goes around the circle, and nobody progresses!

2. Preemption. One thread says "I'm more important, give me that resource".
Analogously, this is like introducing a Firetruck to the intersection, and everyone knows to
give the firetruck the right of way.

Problem: Some threads may not know how to act around a firetruck. We need to enforce the rules.
Firetrucks also have large bumpers desiged to push cars out of the way. Involuntary preemption!
"This thread just kills the other thread and takes the resource by force."

3. Resource ordering. Order (i.e. number, alphabetize) the resource types and require that each
thread acquires resources in an increasing order. All threads must acquire resources in the same
order.

This  doesn't fit our analogy, because it's like having a car throw pilons around the intersection
and saying, "I call this spot!" but it makes sense when we consider threads and locks.
e.g. Car from the north acquires A, then C.
     Car from the East then tries to acquire A and B, but blocks on A.
     Car from the West tries to acquire C then D, and blocks on C.
     Car from the south acquires B then D.
Note: We won't deadlock if South goes first, acquires B and D, and East/West acquire A and C
respectively, and block waiting for South to release B or D. South has all the resources it
needs and will eventually release them - it isn't deadlocked. This is expensive, but it works.

Alternatively: Deadlock detection and recovery. This approach is commonly used in databases.
Since deadlocks are relatively rare, and since database transactions are atomic (can be safely
aborted and restarted later), this works.
We can detect a deadlock and, say, terminate the "least useful" thread.

Now let's talk about the traffic problem in A1.
Conditions for entering the intersection:
 - Imagine the road has infinite lanes from each side. If all cars are going from South to north or
   North to South, all of them can go.
 - If the destination is different, and one or both of the cars are making a right turn, that's okay.
We must implement an enter function and an exit function.
When entering, check: "Three hints."
 - If no other cars are in the intersection, we can go.
 - If there are cars in the intersection:
   - If the other cars are going the same direction as your, you can go.
   "Keep track of the source and destinations of cars in the intersection."
 - When you leave the intersection, you might want to wake up cars that were waiting for you to exit.
 "Selectively wake up cars that were specifically waiting on you."
 "Keep track of how many cars are waiting in any particular direction, to satisfy the fairness
  requirement of the question. We need to keep them cycling through."
CS 350 Jan 28

Remember for A1: Condition variable implementation is very similar to the existing binary semaphore implementation.
Warning: This lecture is kinda dry because it has lots of assembly, but it's important for A2a!
Recall: Three things we can do to avoid deadlocks.
 1. No hold and wait (instead, release resources, back out, and try again later).
    But then we can get a "synchonized balet" of programs attempting to grab resources,
    backing out, trying again, etc. This is a live lock.
 2. Preemption. Kill lower-priority threads. "The most common way to take something away from someone is
    to kill them! :P" "Like a firetruck."
 3. Resource ordering. This avoids cycles in our resource allocation.
    Easy way to illustrate: resource allocation graph...

Box with a dot: resource type, dot = instance.
Circle: thread

A    B
[.]  [.]

O    O
T1   T2

Arrow from A to T1 (T1 acquires lock for T1).
Arrow from T1 to B (T1 is waiting on resource B).
Arrow from B to T2 (T2 acquired lock for B).
Arrow from T2 to A (T1 waiting on lock for A).
A cycle!

With resource ordering:
Arrow from A's dot to T1 (T1 acquires lock for A).
Arrow from T2 to A's box (T2 waiting on A).
Arrow from B's dot to T1 (T1 acquires B).
When T1 finishes, it releases A and B (erase those two arrows).
T2 now acquires A (extend arrow to A's dot).
Arrow from B to T2 (B now acquires T2, too).

Q: Does the order of releasing locks matter?
A: No. T1 can release A and B in any order.

New section: Processes and the Kernel

What's a process?
A process is different from a thread.
A process is an abstraction of a program in execution.
We can think of a program (word processor, etc.) as a process.
A thread is an abstraction of sequential execution, where a process is an abstraction of a *program*.
A process is like a container that can hold one or more threads. It always has at least 1.
In OS/161, we only support one thread per process, but "real" operating systems can have more.
A process is also a continer for other resources, e.g. address space (an allocation of shared memory for this
process)
Every process has its own address space, at least one thread for execution, meta-data for files/sockets opened
by the program. When a process dies, it cleans up those resources.

Multiprogramming "a formal term, similar to multithreading"
Multiple processes existing or running at the same time.
Key idea: multiple processes must share resources. One resource is memory, which we share by assigning
address spaces - we access these via system calls from our process.
Key feature of precesses: isolation. A thread inside process 1 can never go and modify memory that is in process 2.
"It's like each thread in a process is in a different universe and can't read the memory of other address spaces."

The OS Kernel
The kernel is a program, and a core part of the operating system.
It has code and data just like normal programs, but runs in privileged mode.
Other programs don't do this - they run in "user mode".
We don't want our user programs talking to our hardware!
 - isn't isolated
 - lets untrusted programs modify the resources of other programs
 - we want user programs to have only certain permissions (e.g. network access should be blocked with a firewall)
 - if we let a user program write to the network card, it can talk to anyone!
 - the kernel address space must be isolated from other programs (this would give privileged execution to other
   programs!)
Privileged mode also has system-wide instructions like halt (to shut down), manipulate processor state (like
the execution mode), and access to otherwise restricted memory.
User programs can only have limited interaction with hardware via system calls.
"Don't think of the kernel as a process - we will exaplin this more later."

System Calls
The only way to voluntarily ask the kernel to do something for us is through a system call.
e.g. fork, waitpid, getpid, exit (must be implemented in A2a!) (one more (execv?) in a2b)
"Assignment 2a has the largest learning curve, imo."
e.g. sbreak - segment break, changes size of memory segment
How does a system call work?
System calls look like functions. In our program, we call a function in our assembled code by jumping and linking
to a specific location (after setting up registers to pass parameters).
However, these system calls are stored in the kernel address space (in order to prevent other programs from
modifying them).
But it's impossible to jump into kernel memory - we don't have the required privileges!
MIPS (and others) provides a syscall instruction, which:
 1. Switches the processor into system (privileged) mode (but we must keep it contained to system calls).
    It transfers control flow to a special location in memory, where we continue execution
    This is kinda like an interrupt handler! In fact, this is called the system call handler.
    Interrupts, exceptions, and system calls are all handles by the common exception handler - they are all
    similar!
    RFE (Return From Exception) is later run when the system call handler returns, which removes the thread's
    privileges as we return the value to user mode.
 2. Key parts of the current thread context, like the PC, are saved (trap thread) by the common exception
    handler. CEH calls MIPS trap.
    MIPS trap determines whether it was triggered by an exception, sys call, etc.
    If sys call: system call handler services the system call
 3. ...

Q: Why not jsut protect the kernel address space by making it read-only?
A: System call code most likely have to modify kernel resources; temporary local variables are stored on the
   kernel's stack (rather than the user program's stack).
Q: Why not have a different level of privilege where a certain privileged mode only has read access?
A: That is possible, but not in OS/161. We will keep it simple with privileged or unprivileged.

Common exception handler callls MIPS trap function.
MIPS trap determines whether it was an interrupt, exception or system call.

Graphically:

Process          Kernel
   |
   | sys call
   '----------------|
                    |
    sys call return |
   -----------------|
   |
   |
   \/

If you implement this as-is, we would have to write assembly whenever we make system calls, which is a pain in the
butt. So systems provide user-space system call library - a very thin layer that will translate your system call
procedure for you. This is a system call software stack:

Application
----------------
Syscall Library      Unprivileged mode
--------------------------------------
Kernel                 Privileged mode

OS/161 'close' System Call Description
#include <unistd.h>
#include <errno.h>
int main() {
  int x;
  x = close(999);
  if (x < 0) {
    return errno;
  }
  return x;
}

In assembly:
00400050 <main>:
addiu sp,sp-24
sw ra, 16(sp) // store registers
jal ... <close> // jump to close function "does its magic in the kernel then returns to bgez"
                // the close instruction is in user space, and takes one parameter (in a0)
li a0, 999 // parameter passed into a0, in delay slot of jump instruction
bgez v0, 400070 // branch if v0 greater-than-or-equal-to zero
nop
lui v0, 0x1000 // load upper - address of errno into v0
lw v0, 0(v0) // load value of errno into v0

Note: search for "MIPS assembly" for references.

OS/161 System Call Conventions
Integer system call code should be in R2 (v0)
Args should be in R4, R5, R6, R7 (a0, a1, a2, and a3)
When the system call returns, R7 (a3) will be 1 if the call failed, else 0 (success).
R2 (v0) will store the return value of the system call return value if it succeeded, else it will store
the error code for our system call if the call failed.
List of numbers: kern/include/kern/syscall.h
Note: kern/include/kern/ has defninitions for lots of things that must be know by the user space AND the kernel.

What does the user space library look like?
Note: this is all automatically generated, not handwritten
004001dc <close>:
j 4000c0 <__syscall> // jump to syscall label

<__syscall>:
syscall
beqz a3, 400dc <__syscall+0x1c> // jump to return if call succeeded (a3==0)
// else, fetch the error code and store it to v0, and then store -1 to a3
Note: this is all user code, it just makes things easier for us

OS/161 MIPS Exception Handler

common_exception:
First three instructions: check whether we are coming from user space or from the kernel
(an interrupt can be raised from inside the kernel)
Last block of lines: Just trying to find the kernel stack for this particular thread.
"Every single thread actually has two stacks - one in user space, and one in kernel space."
Then, we save the thread context onto the kernel-space stack for this thread.
"It's okay, we never have to touch assembly in this class!"
"Functions like TestAndSet are provided!"
The moment the program jumps from user-land to the kernel, all of its context is stored in the kernel
stack until we leave. This is because:
 1) User stack is not guarunteed to have space
 2) Sensitive data from inside the kernel shouldn't be leaked out
The common exception code then:
 - allocates a trap frame (on the kernel stack!)
 - calls MIPStrap to process the exception
 - when mips_trap returns, we restore the application processor state from the trap frame to the registers

System Call Handler code (called by mips_trap):
 - parameter is a pointer to the trap frame for this particular thread (bottom of the kernel stack)
 - the system call handler has access to the values within the trap frame, so that it can determine which
   system call has been called (it needs to check v0 from within that trap frame - the value is still 49)
    - v0 has probably changed since we called the common exception handler, mips_trap, etc.
 - we then do a switch statement on the call number, and make the appropriate system call
 e.g. case sys__reboot:
  err = tf->tf_a0 // look inside trap frame's saved a0 for the parameter
 - outside of the switch statement, we handle errors (set v0 to err, and set a3 to 1)
 - if no errors happened, set v0 to return value and a3 to 0
 - increment tf->PC by 4 (one instruction) so that we execute the isntruction AFTER the syscall
   after returning (otherwise we would loop on the syscall)

Spend time looking at the syscall implementation for A2a! We add stuff here!
CS 350 Jan 5 2016

A0 will be out later today, will be due "relatively soon"
System call - how applications talk to OS
Assignment portion is heavy - depends on how comfortable you are with the assignments and writing C code
Each assignment can be a couple hundred lines of code.
Debugging is tricky - start assignments early!

Why is concurrency important?
Increasing clock speed isn't ideal and we hit a ceiling around 4GHz.
We need to do lots of things at once without adding heat and battery usage with a higher clock speed.
"Our battery life would suck and we are burning a hole in our pants."
Better CPU architecture can also increase computing speed, but innovations there are difficult.
Google says that students need to be better at concurrent programming.
Varak(?) 25 - medical device in the 80s - two modes, chest xray and radiation therapy. Older version had a mechanical switch. The 25 had a software switch, and the designers had a counter variable auto-increment when booted. Low numbers mean the machine is booting, and user input gets ignored. So unlucky people who pressed the button when the counter hit an integer overflow got the wrong dosage of radiation.
Mars rover (2003, 2004) - rover had small amount of storage for photos. It enterred a reboot loop when its storage ran out of space by keeping meta-data from deleted photos.
We will learn about file systems in this course!

Assignments done individually.
Exams are more important than assignments! Don't neglect studying by working on assignments too long.
Assignments are cumulative - A2 depends on A1, etc.
5 total slip days, at most 3 for each assignments ("days", not "business days")
Only two slip days on last assignment (A3)?
A2a tends to be the hardest, followed by A3 and A2b

Plagiarism - don't use public GitHub repo, use git.uwaterloo.ca.
Don't take code from blogs or other solutions! That makes Bernard feel bad.

Course Intro

What is an Operating System?
Three views:
 - Application view - what services does it provide? Lets software do certain things.
 - System view - what problems does it solve? Ensure important programs have priorty access to resources, etc.
 - Implementation view - how is it built?
 
Application view
OS provides an execution environment - processor time, memory space, I/O, network, GPU, etc.
Provides an interface to these resources - useful abstractions to many potential hardware setups.
DOS didn't provide these abstractions, so applications had to implement many different hardware drivers. This doesn't scale!
OS also isolates running programs from one another - no undesirable interactions.
We need to protect our program's memory from malicious/buggy programs. Lots of potential incompatabilities.

System view
OS enforces resource restrictions - one network card, only so much memory, etc.
Must give the right resources to the right programs.
Policies are difficult to enforce without an OS - only certain programs should be accessing the network, etc.
"OS is a very sophisticated manager of resources."

Implementation view
OS is a concurrent, real-time program. NOt a simple sequential program.
Must handle different events happening at the same time, different device components running and generating events at the same time.
"Your OS only has a certain amount of time to read a packet from the network buffer."
OS must deal with events with very strict time constraints.

Some Terminology
Kernal: OS kernal is core part of the OS that handles system calls, interrupts, and exceptions.
Function calls to included/linked libraries use pre-defined "system calls", special programs that your program can call to talk to the kernel.
Interrupts - service notice, shutdown, etc.
Exceptions - bad/illegal behaviour, kernel kills the program.

Operating System - includes the kernel and other related programs (services, utilities, libraries, interpreters).

Top blobs on slide 5 are user programs. Bottom ones are system resources.
Between them is the kernel.
User programs talk to the kernel via system calls.
Kernel sends commands and data to the system resources, which return data and interrupts.
Then, the system call returns a value to the user program.

Operating System Abstractions
Gives each program its own individual "sandbox" - files and filesystems, address spaces, processes, threads, sockets, pipes.

Why are these abstractions designed the way they are? How are they implemented?

Course outline...
Next class: concurrency.CS 350 Lec 2 Jan 7

Review: Program Execution
Registers
 - program counter, stack pointer...
 - PC contains address of next instructions
 - Jump, branch, etc. change PC (or it auto-increments by 4)
 - Other registers are input params or output from functions
 - Stack pointer points at the top of the stack
 - Program stack contains procedure activation records (call stack, local vars, etc.)
Memory
 - program code
 - program data
 - program stack containing procedure activation records
CPU
 - fetches and executes instructions (fetch, execute loop)
 
 Threads
 A thread represents the control state of an executing program.
 An abstraction of the execution of a sequential program.
 Each thread runs a sequential program one instruction at a time.
 Thread context is the state information needed to run the program.
 "Save everything that this thread touches."
 "Basically a copy of all the registers, but we'll talk more about what needs to be saved later."
 Context consists of:
  - CPU state
  - A stack located in the address space of the thread's process (local vars, return addrs, etc.)

See thread context diagram on slide 3 of this module.

Concurrent Threads
In a single-core CPU, running multiple threads would require something called context switching.
Only one of them can be running at a time.
See next diagram on slide 5.
One set of CPU registers => single-core CPU.
Thread 1 is populating the registers => thread 1 is running.
Thread 2 is paused and waiting, and its context has been stored in the program data.
The thread library manages this context storage.
Also note that there is an extra stack for the second thread.
Students tend to forget this: each thread has its own stack!
Is it possible to have two threads sharing one stack?
Picture a stack growing downward, starting with function A with function B called below.
Then the other thread calls function C and D and those go below B.
If we go back to thread 1, the memory below D is now the top of the stack. Bad things happen.
Note that thread 1 would have no idea what happens, and tries to use the stack pointer as usual.
If thread 1 were to try accessing a local variable in B, it would actually look at D's memory! Oh noes!
"But the stack pointeris a register. It gets stored in the thread's context, so the old stack pointer
 gets restored, and the correct reference is maintained. That problem goes away, but other problems still
 exist."
Two stacks would be interweaved/overlapped. It gets very complicated.
Other single-stack solutions are hacky and inneficient - motivating separate stacks.

Thread Interface for OS/161
A thread library implements threads.
thread_fork is used to create a new thread. Note that (func*)(...) is a function-pointer parameter.
 - This doesn't automatically execute the function, just sets of the thread.
 - Note that thread_fork's function param has a specific format: returns void, first arg is void*, second arg is ulong.
thread_exit destroys and cleans up a thread.
 - note that in thread_exit(void), void means that the function takes no parameters.
thread_yield is where a thread says "I'm not done yet but I'll let another process have the CPU for a while". "voluntary context switch"

/************************************************** 
Thread creation example: cat and mouse (from slide)
**************************************************/

for (index = 0; index < NumMice; index++) {
    error = thread_fork("mouse_simulation thread", NULL, mouse_simulation, NULL, index);
    if (error) {
        panic("mouse_simulation: thread_fork failed: %s\n",
        strerror(error));
    }
}

/* wait for all of the cats and mice to finish */
// "signalling primative used here to wait for all threads to finish"
for(i=0;i<(NumCats+NumMice);i++) {
    P(CatMouseWait);
}

/* This function is going to be passed to thread_fork */
static void mouse_simulation(void * unusedpointer, unsigned long mousenumber)
{
    int i; unsigned int bowl;
    for(i=0;i<NumLoops;i++) {

        /* for now, this mouse chooses a random bowl from
        * which to eat, and it is not synchronized with
        * other cats and mice
        */

        /* legal bowl numbers range from 1 to NumBowls */
        bowl = ((unsigned int)random() % NumBowls) + 1;
        mouse_eat(bowl);
    }

    /* indicate that this mouse is finished */
    V(CatMouseWait);
    /* implicit thread_exit() on return from this function */
}

/***************************************************/


Context Switching, Scheduling, and Dispatching
(Thread) context switching is the act of pausing the execution of one thread and resuming another.
When we context switch:
1. Decide which thread to run next "select"
2. Save context of currently running thread "save"
3. Restore the context of the currently running thread "restore"
Dispatching is just saving context of current thread and restoring the context of the thread being "dispatched".
Sounds simple, but is actually tricky - it's architecture-specific, must be done carefully, and tricky to understand
what is actually happening.

Scheduling
Scheduling is deciding which thread to run next.
Scheduling policy algorithm impacts machine performance a lot.
Simple model: round robin. New threads get put at the end of a "ready queue" that is FIFO.
Scheduler pops the first item of of this queue to run next, and pushes the current thread to the back of the queue.

Causes of Context Switches
1. A thread decides to call thread_yield and voluntarily gives up the CPU.
   This tells the OS to let someone else to run.
   e.g. If a thread is "far ahead" of other threads and wants them to catch up, maybe if it needs resources from
        another thread, it might call thread_yield.
2. A thread calls thread_exit (just a context switch but the current thread isn't queued again).
3. A thread blocks - it is waiting for an event to happen (like waiting for a network packet) - and sleeps until
   the event to happen. While waiting, you pause, and then are woken up when the packet arrives.
   "Like when you are young, instead of constantly asking "are you there yet?" which wastes time and energy, you go to sleep!"
4. A thread is preempted - "an involuntary context switch". The thread library decides, "hey, you've been running long enough,
   time to run another thread".
   This is important because we have threads that run continuously without calling thread_yield. Relying on thread_yield takes
   control away from the kernel and tends to promote "selfish" programs that hog resources and reduce concurrency.

Preemption
Without preemption, a running thread could run forever! A buggy thread could freeze the system!
Preemption ensures fair access to the CPU for the thread library.
Preemption happens via interrupts.

Interrupts
An event that occurs during the execution of a program.
It is caused by system hardware (like a network card) and tells the OS that the hardware needs to be "serviced".
It transfers control to a specific location in memory - an interrupt handler.
An interrupt handler handles the interrupt
 - it saves the thread context
 - determines which piece of hardware raised the interrupt and performs device-specific processing
 - restores the saved thread context and resumes its execution
The primary hardware device we will use to generate interrupts is a timer.
Once a certain amount of time passes, an interrupt is raised, and a context switch happens.
This is how we implement preemption in an OS! With a timer.
Note that if only one thread is active in the system, an interrupt just immediately restores that thread.

Implementing Preemptive Scheduling
 - timer interrupts generates every T time units (e.g. 1ms)
 - suppose thread library uses time quantum q = 500t (preempts a thread every 500ms)
 - to implement this, keep a variable of runningtime that increments with each timer interrupt
 - when a thread is initially dispatched, runningtime is set to 0
 - if runningtime == q, we preempt the thread
